Write in the form of a list of steps to implement this project that aims.  To analyze global sentiment and reactions towards the Saudi Horse Cup

 If this code, 

 Remember, do not explain any part of the code, just mention the steps and touch on the methodology and important things






# -*- coding: utf-8 -*-
"""final_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cz-j70Fc0mqfAbbrprN8wKa_c0rNepFk
"""

!pip3 install transformers
!pip3 install evaluate
!pip3 install torch
!pip3 install protobuf==3.20.3
!pip3 install snscrape
!pip install transformers[torch]
!pip install accelerate -U
!pip install twscrape

import csv
import os
from datetime import datetime
import asyncio
from twscrape import API, gather
from twscrape.logger import set_log_level
import nest_asyncio
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import numpy as np
import re
import torch
from sklearn.model_selection import train_test_split
from huggingface_hub import notebook_login
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TextClassificationPipeline
from datasets import Dataset
import evaluate
from tqdm.notebook import tqdm
from tqdm import tqdm
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from transformers import TextClassificationPipeline
import evaluate

"""# **Data Collection**"""

nest_asyncio.apply()
async def main():
    api = API("accounts.db")
    await api.pool.add_account("example1", "password123", "example1@gmail.com", "password123")
    await api.pool.add_account("example2", "password123", "example2@gmail.com", "password123")

    # await api.pool.add_account("MisterMohammed0", "009988Ppooii@@", "mohammedfoud30@gmail.com", "009988Ppooii@@")
    # await api.pool.add_account("mfoud2023", "009988Ppooii@@", "mfoud2023@gmail.com", "009988Ppooii@@")
    # await api.pool.add_account("mohammed_f14714", "009988Ppooii@@", "mfoud444@gmail.com", "009988Ppooii@@")
    await api.pool.login_all()
    set_log_level("DEBUG")


if __name__ == "__main__":
    asyncio.run(main())

#Get list of accounts and their statuses
!twscrape accounts

async def main():
    api = API("accounts.db")

    # Initialize CSV file
    csv_file_path = 'new.csv'
    fieldnames = [
        'id',   'created_at', 'lang', 'text', 'reply_count',   'hashtags','view_count', 'user_id', 'user_username', 'user_screen_name','user_followers_count', 'user_location'

    ]

    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        query = '#SaudiCup   since:2024-02-20 until:2024-03-05 lang:en'
        async for tweet in api.search(query,  limit=5000):
            tweet_data = {
                'id': tweet.id,
                'created_at': tweet.date.isoformat(),
                'lang': tweet.lang,
                'text': tweet.rawContent,
                'reply_count': tweet.replyCount,
                'like_count': tweet.likeCount,
                'hashtags': ','.join(tweet.hashtags),
                'view_count': tweet.viewCount,
                'user_id': tweet.user.id,
                'user_username': tweet.user.username,
                'user_screen_name': tweet.user.displayname,
                'user_followers_count': tweet.user.followersCount,
                'user_location': tweet.user.location,

            }
            writer.writerow(tweet_data)


    set_log_level("DEBUG")

if __name__ == "__main__":
    asyncio.run(main())

"""## **Data  Visualization and Preprocessing**

"""

df = pd.read_csv('dataset.csv')

df.columns

df.info()

df.head()

df.describe()

df.dtypes

unique_ids = df['id'].unique()

# display the total number of unique IDs
print(f"Total unique IDs: {len(unique_ids)}")

# remove duplicates data
df = df.drop_duplicates(subset='id', keep='first')

#  Pie chart of tweet languages
lang_counts = df['lang'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(lang_counts, labels=lang_counts.index, autopct='%1.1f%%')
plt.title('Tweet Languages')
plt.show()

#  Keep only English language tweets and Pie chart of tweet languages
df = df[df['lang'] == 'en']
lang_counts = df['lang'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(lang_counts, labels=lang_counts.index, autopct='%1.1f%%')
plt.title('Tweet Languages')
plt.show()

#  Histogram of tweet lengths
tweet_lengths = df['text'].str.len()
plt.figure(figsize=(10, 6))
plt.hist(tweet_lengths, bins=30, edgecolor='black')
plt.xlabel('Tweet Length')
plt.ylabel('Frequency')
plt.title('Distribution of Tweet Lengths')
plt.show()

# Convert 'created_at' column to datetime format
df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S %z %Y')


#  Line plot of tweet count over time
df['created_at'] = pd.to_datetime(df['created_at'])
tweet_counts = df.resample('D', on='created_at').size()
plt.figure(figsize=(12, 6))
plt.plot(tweet_counts.index, tweet_counts)
plt.xlabel('Date')
plt.ylabel('Tweet Count')
plt.title('Tweet Count Over Time')
plt.xticks(rotation=45)
plt.show()

hashtags = df['hashtags'].str.cat(sep=' ')
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(hashtags)
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Hashtags')
plt.show()

# Fill missing values in 'user_location' with 'Unknown'
df['user_location'] = df['user_location'].fillna('Unknown')

# Horizontal bar plot of top 10 countries by user count
user_country_counts = df['user_location'].value_counts().nlargest(10)
plt.figure(figsize=(10, 8))
plt.barh(user_country_counts.index, user_country_counts)
plt.xlabel('Number of Users')
plt.ylabel('Country')
plt.title('Top 10 Countries by User Count')
plt.show()

# Data Visualization : Bar plot of top 10 users by follower count
top_users = df.nlargest(10, 'user_followers_count')
plt.figure(figsize=(12, 6))
plt.bar(top_users['user_screen_name'], top_users['user_followers_count'])
plt.xlabel('User Screen Name')
plt.ylabel('Followers Count')
plt.title('Top 10 Users by Follower Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# **Sentiment Analysis**"""

!wget https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip
!unzip trainingandtestdata.zip

class DataLoader():

    def import_tweets(self, path, cols=[0, 5]):
        self.tweets_df = pd.read_csv(path, usecols=cols, names=["label", "text"], encoding='latin-1')
        self.num_tweets = self.tweets_df.shape[0]

        self.tweets_df["text"] = self.tweets_df["text"].astype(str)
        self.tweets_df["label"] = self.tweets_df["label"].astype(str)

        return self.tweets_df

    def split_tweets(self, train_size, test_size):
        self.tweets_train, self.tweets_test = train_test_split(self.tweets_df, train_size=train_size, test_size=test_size)
        return self.tweets_train, self.tweets_test

class TwitterSentimentClassifier():

    def __init__(self, model_name='Twitter/twhin-bert-base', num_labels=2, label2int={"0":0, "4":1}, train_size=100000, test_size=10000):
        self.model_name = model_name
        self.num_labels = num_labels
        self.train_size = train_size
        self.test_size = test_size
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.label_to_int = label2int
        self.metric_accuracy = evaluate.load("accuracy")
        self.metric_precision = evaluate.load("precision")
        self.metric_recall = evaluate.load("recall")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def tokenize_function(self, batch):
        tokenized_batch = self.tokenizer(batch["text"], padding='max_length', truncation=True, max_length=140)
        tokenized_batch["label"] = [self.label_to_int.get(label, None) for label in batch["label"]]
        return {k: v for k, v in tokenized_batch.items() if k != "label" or v is not None}

    def import_tweets(self, path):
        dl = DataLoader()
        dl.import_tweets(path)
        self.train, self.test = dl.split_tweets(self.train_size, self.test_size)

        self.train = Dataset.from_pandas(self.train)
        self.test = Dataset.from_pandas(self.test)

        self.train_dataset = self.train.map(self.tokenize_function, batched=True)
        self.test_dataset = self.test.map(self.tokenize_function, batched=True)

    def get_metrics(self, predictions, labels):
        accuracy = self.metric_accuracy.compute(predictions=predictions, references=labels)["accuracy"]
        precision = self.metric_precision.compute(predictions=predictions, references=labels)["precision"]
        recall = self.metric_recall.compute(predictions=predictions, references=labels)["recall"]
        f1_score = 2 * (precision * recall) / (precision + recall)

        return {"accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1": f1_score}

    def compute_metrics(self, eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return self.get_metrics(predictions, labels)

    def train_model(self):
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=self.num_labels)
        self.model.to(self.device)
        self.training_args = TrainingArguments(
            report_to="none",
            output_dir="training_arguments",
            save_strategy="no",
            evaluation_strategy="epoch",
            per_device_train_batch_size=32,
            per_device_eval_batch_size=32,
            learning_rate=3e-5,
            weight_decay=0.01,
            num_train_epochs=3,
        )

        self.trainer = Trainer(
            model=self.model,
            args=self.training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.test_dataset,
            compute_metrics=self.compute_metrics,
        )

        self.trainer.train()

    def save_model_local(self, path):
        self.trainer.save_model(path)

    def save_model_cloud(self):
        raise NotImplementedError("not currently implemented")

    def load_saved_model(self, path):
        self.model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=self.num_labels)
        self.model.to(self.device)
        self.pipe = TextClassificationPipeline(model=self.model, tokenizer=self.tokenizer, device=0 if self.device.type == "cuda" else -1)

    def get_scores(self, text):
        return self.pipe(text)

    def predict_dataset(self, dataset_path, batch_size=32):
        # Load the dataset
        df = pd.read_csv(dataset_path)

        # Ensure there's a 'text' column
        if 'text' not in df.columns:
            raise ValueError("The dataset must contain a 'text' column")

        # Make predictions in batches
        predictions = []
        for i in tqdm(range(0, len(df), batch_size), desc="Predicting"):
            batch = df['text'][i:i+batch_size].tolist()
            batch_predictions = self.predict(batch)
            predictions.extend(batch_predictions)

        # Add predictions to the dataframe
        df['predicted_sentiment'] = predictions

        # Map numeric predictions back to labels
        label_map = {0: "Negative", 1: "Positive"}
        df['predicted_sentiment_label'] = df['predicted_sentiment'].map(label_map)

        return df

    def predict(self, text):
        if isinstance(text, str):
            text = [text]

        inputs = self.tokenizer(text, truncation=True, padding='max_length', max_length=140, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=-1)

        return predictions.cpu().numpy().tolist()

    def evaluate_model(self, path, cols=[0, 5]):
        dl = DataLoader()
        dl.import_tweets(path, cols=cols)

        # Filter out rows with labels not in label_to_int
        dl.tweets_df = dl.tweets_df[dl.tweets_df['label'].isin(self.label_to_int.keys())]

        self.eval = Dataset.from_pandas(dl.tweets_df)
        self.eval_dataset = self.eval.map(self.tokenize_function, batched=True)

        predictions = np.array(self.predict(self.eval_dataset["text"]))
        labels = np.array(self.eval_dataset["label"])

        return self.get_metrics(predictions, labels)

ts = TwitterSentimentClassifier()

dataFile = 'training.1600000.processed.noemoticon.csv'


ts.import_tweets(dataFile)

ts.train_model()

ts.save_model_local("twitter-sentiment")

ts.load_saved_model("twitter-sentiment")

ts.evaluate_model("testdata.manual.2009.06.14.csv")

results_df = ts.predict_dataset("dataset.csv")



# If you want to save the results
results_df.to_csv("predictions_output.csv", index=False)

# Pie chart of sentiment distribution
sentiment_counts = results_df['predicted_sentiment_label'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')
plt.title('Sentiment Distribution')
plt.show()

!zip -r twitter-sentiment-0k.zip  twitter-sentiment-20k