# -*- coding: utf-8 -*-
"""twiter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nz2SCckpaFqSU1vImJE5BIhXyHF334ww
"""

!pip install twikit datetime

from twikit import Client

USERNAME = 'MisterMohammed0'
EMAIL = 'mohammedfoud30@gmail.com'
PASSWORD = '009988Ppooii@@'

# Initialize client
client = Client('en-US')

client.login(
    auth_info_1=USERNAME ,
    auth_info_2=EMAIL,
    password=PASSWORD
)

from datetime import datetime

def is_between_dates(check_date_str):
    # Define the date format
    date_format = '%a %b %d %H:%M:%S %z %Y'
    start_date_str = 'Tue Feb 20 00:00:00 +0000 2024'
    end_date_str = 'Tue Mar 05 00:00:00 +0000 2024'

    # Convert the date strings to datetime objects
    start_date = datetime.strptime(start_date_str, date_format)
    end_date = datetime.strptime(end_date_str, date_format)
    check_date = datetime.strptime(check_date_str, date_format)

    # Check if check_date is between start_date and end_date
    return start_date <= check_date <= end_date

import csv
import os
from datetime import datetime, timezone
import time
from twikit import Client, TooManyRequests

# Define date range
# start_date = datetime.strptime('2024-02-20', '%Y-%m-%d').replace(tzinfo=timezone.utc)
# end_date = datetime.strptime('2024-03-05', '%Y-%m-%d').replace(tzinfo=timezone.utc)

# Set the desired maximum number of tweets to retrieve
max_tweets = 5000

# Define CSV file path
csv_file_path = 'tweets5000Latest.csv'

# Create CSV file and write header if it does not exist
if not os.path.isfile(csv_file_path):
    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow([
               'id', 'created_at', 'created_at_datetime', 'text', 'lang',
    'in_reply_to', 'is_quote_status', 'quote', 'retweeted_tweet',
    'possibly_sensitive', 'possibly_sensitive_editable', 'quote_count',
    'media', 'reply_count', 'favorite_count', 'favorited', 'view_count',
    'retweet_count', 'editable_until_msecs', 'is_translatable',
    'is_edit_eligible', 'edits_remaining', 'state', 'replies',
    'reply_to', 'related_tweets', 'hashtags', 'poll', 'has_card',
    'thumbnail_title', 'thumbnail_url', 'urls', 'full_text',
    'user_id', 'user_created_at', 'user_name', 'user_screen_name',
    'user_profile_image_url', 'user_profile_banner_url', 'user_url',
    'user_location', 'user_description', 'user_description_urls',
    'user_urls', 'user_pinned_tweet_ids', 'user_blue_verified',
    'user_verified', 'user_possibly_sensitive', 'user_can_dm',
    'user_can_media_tag', 'user_want_retweets', 'user_default_profile',
    'user_default_profile_image', 'user_has_custom_timelines',
    'user_followers_count', 'user_fast_followers_count',
    'user_normal_followers_count', 'user_following_count',
    'user_favorites_count', 'user_listed_count', 'user_media_count',
    'user_statuses_count', 'user_is_translator', 'user_translator_type',
    'user_withheld_in_countries'
        ])

def write_tweet_to_csv(tweet, writer):
    user = tweet.user
    writer.writerow([
        tweet.id,
        tweet.created_at,
        tweet.created_at_datetime,
        tweet.text,
        tweet.lang,
        tweet.in_reply_to,
        tweet.is_quote_status,
        tweet.quote,
        tweet.retweeted_tweet,
        tweet.possibly_sensitive,
        tweet.possibly_sensitive_editable,
        tweet.quote_count,
        tweet.media,
        tweet.reply_count,
        tweet.favorite_count,
        tweet.favorited,
        tweet.view_count,
        tweet.retweet_count,
        tweet.editable_until_msecs,
        tweet.is_translatable,
        tweet.is_edit_eligible,
        tweet.edits_remaining,
        tweet.state,
        tweet.replies,
        tweet.reply_to,
        tweet.related_tweets,
        tweet.hashtags,
        tweet.poll,
        tweet.has_card,
        tweet.thumbnail_title,
        tweet.thumbnail_url,
        tweet.urls,
        tweet.full_text,
        user.id,
        user.created_at,
        user.name,
        user.screen_name,
        user.profile_image_url,
        user.profile_banner_url,
        user.url,
        user.location,
        user.description,
        user.description_urls,
        user.urls,
        user.pinned_tweet_ids,
        user.is_blue_verified,
        user.verified,
        user.possibly_sensitive,
        user.can_dm,
        user.can_media_tag,
        user.want_retweets,
        user.default_profile,
        user.default_profile_image,
        user.has_custom_timelines,
        user.followers_count,
        user.fast_followers_count,
        user.normal_followers_count,
        user.following_count,
        user.favourites_count,
        user.listed_count,
        user.media_count,
        user.statuses_count,
        user.is_translator,
        user.translator_type,
        user.withheld_in_countries
    ])

def fetch_tweets(tweets, writer):
    count = 0
    api_calls = 0
    call_limit = 200
    limit_reset_time = 3 * 60  # 15 minutes in seconds

    while count < max_tweets:
        try:
            for tweet in tweets:
                tweet_date = datetime.strptime(tweet.created_at, '%a %b %d %H:%M:%S %z %Y')
                write_tweet_to_csv(tweet, writer)
                count += 1
                if count >= max_tweets:
                    return count
                # print(count)

            # api_calls += 1
            # if api_calls >= call_limit:
            #     print("Rate limit reached. Sleeping for 15 minutes.")
            #     time.sleep(limit_reset_time)
            #     api_calls = 0
            print("next")

            tweets = tweets.next()
        except TooManyRequests:
            print("Rate limit exceeded. Sleeping for 15 minutes.")
            time.sleep(limit_reset_time)
            api_calls = 0
        except StopIteration:
            break

    return count
try:
# Initial search
  tweets = client.search_tweet('#saudicup lang:en', 'Latest' ,100)
  # Append tweets to CSV file
  with open(csv_file_path, mode='a', newline='', encoding='utf-8') as file:
      writer = csv.writer(file)
      count = fetch_tweets(tweets, writer)
      print(f"Total tweets fetched: {count}")
except TooManyRequests:
    print("Rate limit exceeded. Sleeping for 15 minutes.")
    time.sleep(3 * 60)
    tweets = client.search_tweet('#saudicup lang:en', 'Latest',100)
    # Append tweets to CSV file
    with open(csv_file_path, mode='a', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        count = fetch_tweets(tweets, writer)
        print(f"Total tweets fetched: {count}")

!pip install twitter-api-client -U

"""## **Data Visualization**

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
from wordcloud import WordCloud


df = pd.read_csv('/content/tweets5000Latest (2).csv')

df.head()

df.describe()

df.dtypes

# Data Visualization : Pie chart of tweet languages
lang_counts = df['lang'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(lang_counts, labels=lang_counts.index, autopct='%1.1f%%')
plt.title('Tweet Languages')
plt.show()

# Data Preprocessing: Keep only English language tweets
df = df[df['lang'] == 'en']
lang_counts = df['lang'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(lang_counts, labels=lang_counts.index, autopct='%1.1f%%')
plt.title('Tweet Languages')
plt.show()

# Removing unnecessary columns
columns_to_drop = ['User Profile Banner URL', 'User Description URLs', 'User URLs', 'User Pinned Tweet IDs']
df = df.drop(columns=columns_to_drop, axis=1)

#  Histogram of tweet lengths
tweet_lengths = df['text'].str.len()
plt.figure(figsize=(10, 6))
plt.hist(tweet_lengths, bins=20)
plt.xlabel('Tweet Length')
plt.ylabel('Frequency')
plt.title('Distribution of Tweet Lengths')
plt.show()

#  Line plot of tweet count over time
df['created_at_datetime'] = pd.to_datetime(df['created_at_datetime'])
tweet_counts = df.resample('D', on='created_at_datetime').size()
plt.figure(figsize=(12, 6))
plt.plot(tweet_counts.index, tweet_counts)
plt.xlabel('Date')
plt.ylabel('Tweet Count')
plt.title('Tweet Count Over Time')
plt.xticks(rotation=45)
plt.show()

#  Word cloud of hashtags
from wordcloud import WordCloud

hashtags = df['hashtags'].str.cat(sep=' ')
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(hashtags)
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Hashtags')
plt.show()

# Perform sentiment analysis using TextBlob
df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['sentiment_label'] = df['sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))

# Data Visualization : Pie chart of sentiment distribution
sentiment_counts = df['sentiment_label'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')
plt.title('Sentiment Distribution')
plt.show()

#  Bar plot of sentiment distribution by language
language_sentiment = df.groupby(['lang', 'sentiment_label']).size().unstack()
language_sentiment.plot(kind='bar', figsize=(12, 6))
plt.xlabel('Language')
plt.ylabel('Count')
plt.title('Sentiment Distribution by Language')
plt.legend(title='Sentiment')
plt.xticks(rotation=45)
plt.show()

# Data Visualization : Scatter plot of sentiment vs. retweet count
plt.figure(figsize=(10, 6))
plt.scatter(df['sentiment'], df['retweet_count'], alpha=0.5)
plt.xlabel('Sentiment')
plt.ylabel('Retweet Count')
plt.title('Sentiment vs. Retweet Count')
plt.show()

# Standardize column names
df.columns = df.columns.str.lower().str.replace(' ', '_')

# Data Visualization : Horizontal bar plot of top 10 countries by user count
user_country_counts = df['user_location'].value_counts().nlargest(10)
plt.figure(figsize=(10, 8))
plt.barh(user_country_counts.index, user_country_counts)
plt.xlabel('Number of Users')
plt.ylabel('Country')
plt.title('Top 10 Countries by User Count')
plt.show()

# Data Visualization : Bar plot of top 10 users by follower count
top_users = df.nlargest(10, 'user_followers_count')
plt.figure(figsize=(12, 6))
plt.bar(top_users['user_screen_name'], top_users['user_followers_count'])
plt.xlabel('User Screen Name')
plt.ylabel('Followers Count')
plt.title('Top 10 Users by Follower Count')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Remove unnecessary columns
columns_to_keep = ['text']
df = df[columns_to_keep]

# Load the BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

# Define the sentiment labels
sentiment_labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}

# Perform sentiment analysis using BERT
def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    outputs = model(**inputs)
    predicted_class = torch.argmax(outputs.logits, dim=1).item()
    return sentiment_labels[predicted_class]

df['sentiment_label'] = df['text'].apply(predict_sentiment)

# Save the results to a new CSV file
df.to_csv('sentiment_results.csv', index=False)

print("Sentiment analysis completed. Results saved to 'sentiment_results.csv'.")